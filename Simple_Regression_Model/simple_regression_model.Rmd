---
title: "The Simple Regression Model"
author: "Jamie Woods"
date: "1/22/2021"
output: 
  html_notebook: 
    fig_width: 6
    fig_height: 12

runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



We are going to build up your intuition on how some features of your data generating process express themselves in the sample you draw from that process and the statistics, parameter estimates and the like, that come from your regression estimates.

Keep in mind that we are starting with known population characteristics, something we can't do in real life.

We are envisioning a model like this:

$$Y = \alpha + \beta x + \nu$$

We will alter:

+ The standard deviation of the error^[I'll swap error and residual for a while but when talking about populations use error nd when talking about samples use residuals.  We will solidify that division a little later.] term, $\nu$.
+ $\alpha$, which is the intercept term.
+ $\beta$, which is the slope coefficient on X.
+ $N$, the number of observations in the sample we get from the population.
+ The range of the X variable, which we will draw from a uniform distribution.


Check out the SimpleRegressionData function below.  It is rigged so there are default values for everything.

What it does is take values for alpha, beta, the range of X and the standard deviation of the error term. What this function does is enables us to get as many _samples_ as we like, of any size, from our _population_ distribution, the data generating function, so we can play with it.

```{r}
# Used to generate fake data.   This has no checks that parameters are in the right range.

SimpleRegressionData <- function(alpha = 1, 
                                 beta = 5, 
                                 rangex = c(0, 10), 
                                 sigma = 1,
                                 N = 100)
  {
    ret <- data.frame(X = runif(N, rangex[1], rangex[2]),
                      er = rnorm(N, 0, sigma))
    ret$Y <- alpha + (beta * ret$X) + ret$er
    ret
}

```

Every time you run the function, new data will come out.

+ Run the function in a chunk and produce some summary statistics.  Do this a few times.  

```{r}
library(gtsummary)

SimpleRegressionData(N = 10) %>%
  tbl_summary(statistic = list(all_continuous() ~ "{mean} ({sd})",
                     all_categorical() ~ "{n} / {N} ({p}%)"),)
```

+ Run the function again but this time alter the standard deviation of the error^[Here is another question about using errors or residuals thing.  We are altering the population value so we should talk errors but we are generating residuals.] term to 10.



Now we are going to run some regressions with this fake data.  The widget below generates fake data, plots it, draws a line through it, and gives the usual regression output.

The graph and table will update, generating new data, every time you change a slider so it won't always be smooth transition.

```{r echo=FALSE}
library(shiny)
library(ggplot2)
library(gtsummary)



shinyApp(

ui = fluidPage(height = "100%",

  
  sidebarLayout(
    sidebarPanel(
  sliderInput("Sigma", label = "Sigma:", min = 0, max = 50,  value = 1),

  sliderInput("Alpha", "Alpha:",  min = -10, max = 10, value = 1),

  sliderInput("Beta", "Beta:",  min = -10, max = 10, value = 2),

  sliderInput("NObs", "N:",  min = 5, max = 200, value = 20), 
  
  sliderInput("Rangex", "Range X", min = -10, max = 30, value = c(0,5))
  
    ),
  
  mainPanel(
 plotOutput("lm_plot"),
 
 htmlOutput("lm_result")
  )
  )
),


server = function(input, output){
  
   # simulated_data <- reactive({
   #   SimpleRegressionData(alpha = input$Alpha, beta = input$Beta, rangex = c(input$Startx, input$Startx + input$Rangex), sigma = input$Sigma, N = input$NObs)
   #   
   # })

     
   simulated_data <- reactive({
     SimpleRegressionData(alpha = input$Alpha, beta = input$Beta, rangex = input$Rangex, sigma = input$Sigma, N = input$NObs)
   })

   
      
   output$data = renderPrint({head(simulated_data(), 5)})
 
  output$lm_result = renderPrint({
      lm(Y~X, data = simulated_data())%>% 
      tbl_regression(intercept = TRUE) 
    })

  output$lm_plot = 
    renderPlot({
     simulated_data() %>%
  ggplot(aes(y = Y, x = X)) +
  geom_point() +
  geom_smooth(method = "lm")
    })
  
   # output$data = renderPrint(head(SimpleRegressionData(alpha = input$Alpha, beta = input$Beta, rangex = c(input$Startx, input$Startx + input$Rangex), sigma = input$Sigma, N = input$NObs), 5))
  # 
  
}

)
```

We will get the details on the distribution of the confidence intervals and the p-values in the next section.  Alpha, (Intercept) in the Toy Regression output is the estimated intercept and Beta, X in the Toy Regression output is the slope coefficient.


# Messing with N

+  Change the number of observations, N, in the sample over a wide range.

    + Do the estimates of $\alpha$ and $\beta$ change much over the range?
    
    The estimates don't change that much but they do change. There is sampling distribution.
    
    + Look at the confidence intervals over the range of N you choose, what happens to them as N increases?
    

+ Pick a small N, like 8.  Note the coefficients for $\alpha$ and $\beta$. Move the slider back and forth between 8 and 9, or as small a change as you can make.  This generates new samples from the population.

    + What happens to the estimates of $\alpha$ and $\beta$?
    + How wide is the confidence interval?  Does it change much?
    
+ Pick a larger N, like 100.  Note the coefficients for $\alpha$ and $\beta$. Move the slider back and forth between 100 and 101, or as small a change as you can make.  This generates new samples from the population.

    + What happens to the estimates of $\alpha$ and $\beta$ when you get a new sample?
    + How wide is the confidence interval relative to when N was small?
    +  Does you confidence interval change much relative to when N was small?
    
## Discussion

As N increases parameter estimates get more stable from sample to sample.  They don't move much.

As N increase, your confidence intervals get narrower.  They still change from sample to sample but not as much.


# Messing with $\sigma$

What we are going to do now is play with the part of the data we can't observe, the unsystematic part.  It is a grand summary of the effects of everything that is not in the model.   We assume that:

$$E(u|x) = E(u) =0$$

Set your parameters to some reasonable values, $\alpha = 1$ and $\beta = 2$, and have N at about 30 or so.

+ Set sigma to zero.  What do you see and intuitively what does that mean?

+ Set sigma to something small, like one, and then compare it to a large sigma like 50.  What changes do you see in sizes of the confidence intervals?

+ Set sigma to the maximum then move the slider back and forth between 50 and 49, or as small a change as you can make.  This generates new samples from the population.  See if you can generate both positive and negative slopes.

## Discussion

As the standard deviation of the error term increases, the confidence intervals get wider.

# Messing with the range of X

Don't get stuck with the idea that the Xs, the right-hand side variables, don't matter that much.  The variance of the exogenous variables, $var(x)$, is a big deteriminant of the regression coefficients. 

$$\hat{\beta} = \frac{cov(x,y)}{var(x)}$$

Set your parameters to some reasonable values, $\alpha = 1$ and $\beta = 2$, and have N at about 30 with $\sigma =1$ or so. 

+  Set the range of x so that the high and low value are the same.  What happened?

    + Why is there no parameter estimate for $\beta$?
    + Why is there still a parameter estimate for the intercept?
    + Set the N to the maximum.  What happens to the estimate for the intercept?
    
+ Put N back at about 20 and set the lowest value of X to -10 and then start increasing the upper bound from there.

    + What happens to the confidence interval for X, i.e, the $\beta$?


## Discussion


YOUR NOTES HERE


# The Power of One Observation

You will mostly be looking at data where one observation looks similar to others with about the same conditional means and so on.  Every so often you get a few odd ball observation.  Sometimes these are coding errors.  There are some typical problems to look for including: 

+ Flipped digits, "912" coded as "192".
+ Factor of 10 errors, decimal in the wrong place, "10.0" coded as "100"


Sometimes the odd observations are because of some unmodeled, i.e., subsumed into the error term, event.  If the event is so rare that it only happens only a small number of times in your data set. This one observation can make large changes in your parameter estimates. 

The toy below allows you to move one data point. The data is generated as it was before with $Y = 0 + 0 X + \nu$.  The regression line should show as a nearly horizontal line.

The two sliders allow you to move one dot.  It starts at (0,0), $(\bar{Y},\bar{X})$ which is always on the regression line.  You can move the dot left and right and up and down.

```{r echo=FALSE, warning=FALSE}
library(shiny)
library(ggplot2)
library(tidyverse)



shinyApp(

ui = fluidPage(height = "100%",

  
  sidebarLayout(
    sidebarPanel(

  sliderInput("Outlier_x", "Outlier X:",  min = -30, max = 30, value = 0), 
  
  sliderInput("Outlier_y", "Outlier Y:", min = -20, max = 20, value = 0)
  
    ),
  
  mainPanel(
 plotOutput("lm_plot"),
 
 htmlOutput("lm_result")
  )
  )
),


server = function(input, output){
  
  base_simulated_data <-   SimpleRegressionData(alpha = 0, beta = 0, rangex = c(-15, 15), sigma = 2, N = 30)
  
   simulated_data <- reactive({
     base_simulated_data %>%
       add_row(X = input$Outlier_x, Y = input$Outlier_y,er =0)
   })

   
      
   output$data = renderPrint({head(simulated_data(), 5)})
 
  output$lm_result = renderPrint({
      lm(Y~X, data = simulated_data())%>% 
      tbl_regression(intercept = TRUE) 
    })

  output$lm_plot = 
    renderPlot({
     simulated_data() %>%
  ggplot(aes(y = Y, x = X)) +
  geom_point() +
  geom_smooth(method = "lm")
    })
  
  
}

)
```


+ Set the Outlier X slider to zero, i.e, $\bar{X}$.  Vary the Outlier Y.  What happens to the regression output?

+ Set the Outlier Y to something large.  What happens to the regression output as you vary X?

## Discussion


IRL, outliers like that are an indication that there was a data capture problem, like flipping the first two digits or format change.  Sometimes data comes from many sources and may only superficially look the same.  My favorite example was when I was working with some billing data, for a few 10,000 of customers, and the data passed the reasonableness screens.  What happened was that for a small number of households over two months, the date format changed from near ISO YYMMDD to US MMDDYY.  The outlier analysis detected this.

The other thing they do is indicate that you should do some additional investigation.  It could be that you should include another variable, indicating days with tornadoes in one case, or that the data was missing something important, like the existence of restaurants in buildings that were part of a data set.  All these examples were discussed in class. 

# Three Ways to $\beta$

## A Note on R Representation of Functional Form and the lm function.

R has a workhorse function, `lm` that handles linear regression for you.  There are plenty of others that do regression and other thing `sysfit` amd `glm` are the others that do linear regression and more.  

The thing that is tricky about R is the compact way they represent linear regressions. Lets work with a different dataset than we have before to demo this.

```{r}
data("USArrests")
summary(USArrests)
```

You can peek at the variable definitions, they are per 100,00 murders, assaults and rape by state with the percent of the state that is in an urban area.

We can regress the percent urban population on the murder rate like this.  When you run this you get just the parameter estimates and little else.  The `~` acts as the equal sign separating the RHS from the LHS.

```{r}

lm(Murder ~ UrbanPop, data = USArrests)

```
We can store the results in a new object, `lm_murder_pop`, lm for the model, murder for the lhs and pop for the rhs. and then use the `names` function to show you what is stored.

```{r}
lm_murder_pop <- lm(Murder ~ UrbanPop, data = USArrests)

names(lm_murder_pop)
```

You can get at this with the usual `$` operator.

To see the usual regression output you can wrap the `lm` call in a `summary` or you can use pipes to send results around.  These all do the same thing, estimate a regression and show you the results through summary.  Everything in the chunk below does the same thing, just in a different way.  The outputs will be identical.

```{r}

summary(lm(Murder ~ UrbanPop, data = USArrests)) # all on one line and result is not stored.


# Storing results for later is common when it takes more than 1/10th of a second for the results to appear.

lm_murder_pop <- lm(Murder ~ UrbanPop, data = USArrests) # Do the regression and store it.
summary(lm_murder_pop)  # give the standard output.

# These are dplyrish methods

lm_murder_pop %>%
  summary()


# This one starts with the data and pipes it to a regression and then to summary.
# the . is the only odd part.  Pipes usually assume they go to the first argument but
# the . says, put it here.

USArrests %>%
  lm(Murder ~ UrbanPop, data = .) %>%
  summary()

```

The formula argument is a reasonably flexible and compact way of expressing the regression equation. The simple `Murder ~ UrbanPop` we used above translates to:

```{r results = "asis"}
library(equatiomatic)

extract_eq(lm_murder_pop)

```

You can do some simple transformations, like square and logs or even add variables together too.

This logs the LHS
```{r results = "asis"}
lm(log(Murder) ~ UrbanPop, data = USArrests) %>%
  extract_eq()
```


If you want to do math on the rhs, you need to wrap the variable in an `I()` function.  This describes the log of the murder rate with the square of the percent urban population.

```{r results = "asis"}
lm(log(Murder) ~ I(UrbanPop^2), data = USArrests) %>%
  extract_eq()
```

If you want to add another variable on the RHS just append it with a `+`

```{r results = "asis"}
lm(Murder ~ UrbanPop + Assault, data = USArrests) %>%
  extract_eq()
```

Note that the regression always includes and intercept term, the $\alpha$ unless you tell it not to include one with a `-1`.

```{r results = "asis"}
lm(Murder ~ -1 + UrbanPop + Assault, data = USArrests) %>%
  extract_eq()
```

There are ways to do more complex things, like interact variables, but we will put those off till later.  R handles some low level tasks, like squaring and logging and constructing dummy variable series, for you so you don't have to construct new variables before you create a regression.  You will have to get used to doing this on your own if you learn  SAS or Stata in the future.

Lets Id the outputs now.

```{r}
USArrests %>%
  lm(Murder ~ UrbanPop, data = .) %>%
  summary()


```


Starting at the top.

+ Call:  Is just a neatened up version of how you wrote the lm function call.
+ Residuals:  The min max and IQR of the residual terms.  You know the mean is zero by definition.
+ Coefficients:  This is your meat.  Each parameter is named after the associated data column or the intercept.  The other columns are:
    + Estimate: What is says on the box.  These are the means, remember the estimates are random variables, of slope and intercepts.
    + Std. Error: The estimates are random variables so they are known with uncertainty.  This is the standard deviation of those estimates.
    + t value:  Calculation done for you.  It is the number of standard deviation the estimate is away from the mean, $\frac{Estimate}{Std. ~Error}$
    + p(>|t|):  Interprets the t value and tells you the probability of observing the estimate or larger when the true value of the parameter is zero.  If this number is small, meaning it is unlikely, there will be some asterisks next to it.
+ Residual standard error:  Tells you a few things.  It tells you the standard error of the $\epsilon$  term and tells you how many degrees of freedom were used to estimate the standard error.  For now, this is just the number of observations, N, minus the number of parameters you have estimated in the regression, 2 in this case, one for the intercept and one for the slope.   

The last two lines are about the regression as a whole, which is the next topic.
    
# F, $R^2$ and Goodness-of-fit

You can think of these two as a pair similar to the parameter estimate and their variance.  $R^2$ is to $\hat{\beta}$ as $F$ is to $t-value$. Both $R^2$ and  $\hat{\beta}$ give an effect size. Both $F$ and the $t-value$ are used to calculate the significance of that effect size.

The real question is what effect is the $R^2$ measuring?  It is basically the fraction of the total variation in the RHS variable explained by the equation or one minus the ratio of the variance of the error term and the LHS variable.  Put another way you are getting a measure of the explanatory power of all the variables that are not the intercept, meaning the mean of the RHS with no other conditioning variables.

There is another regression toy below that allows you to alter both the standard deviation of the error term and the number of observations.

```{r echo=FALSE, warning=FALSE}
library(shiny)
library(ggplot2)
library(tidyverse)



shinyApp(

ui = fluidPage(height = "100%",

  
  sidebarLayout(
    sidebarPanel(

  sliderInput("Sigma", "sigma:",  min = .01, max = 30, value = 1), 

  
  sliderInput("Obs", "N:",  min = 20, max = 1000, value = 20), 

    ),
  
  mainPanel(
 plotOutput("lm_plot"),
 
 verbatimTextOutput("lm_result")
  )
  )
),


server = function(input, output){
  
  base_simulated_data <-   SimpleRegressionData(alpha = 1, beta = 2, rangex = c(-15, 15), sigma = 1, N = 1000)
  
   simulated_data <- reactive({
     base_simulated_data %>%
       mutate(Y = 1 + (2*X) + (er*input$Sigma)) %>%
       head(input$Obs)
   })

   
      
   output$data = renderPrint({head(simulated_data(), 5)})
 
  output$lm_result = renderPrint({
      lm(Y~X, data = simulated_data())%>% 
      summary()
    })

  output$lm_plot = 
    renderPlot({
     simulated_data() %>%
  ggplot(aes(y = Y, x = X)) +
  geom_point() +
  geom_smooth(method = "lm")
    })
  
  
}

)
```


+ Set sigma to something reasonable and observe the $R^2$ and then vary the number of observations.  Does your $R^2$ change much?  What about the F statistic?

+ Set the number of observatons to a moderate value, around 100, and then vary the standard deviation.  What do you observe?

# Changing Units


Changing units, like changing income from dollars to thousands of dollars, is a pretty common exercise.  

There are some good reasons to do it.  All your statistics are calculated in a computer using floating point arithmetic.  You have to be careful with that.  When you add numbers together, which you need to do to calculate your parameter estimates, all the numbers should be on about the same scale.  So, if you have one variable that looks like 0.000005 and another that looks like 50,000,000, there is a good chance your computer will tell you rounding and truncation error lies.

Getting things on the same scale will also help you interpret parameters because the parameters themselves will be on a similar scale.

Changing scale, multiplying a RHS variable by a scalar, is pretty common and does something reasonable.

Lets use the USArrests data again and demonstrate.

Here is the original Murder Regression.
```{r}
USArrests %>%
  lm(Murder ~ UrbanPop, data = .) %>%
  summary()

```


Keep an eye on the parameter estimates.  Now I lets change the data so that instead of urban population being expressed as a percent it is expressed as a fraction, i.e., divide by 100.

```{r}
USArrests %>%
  mutate(UrbanPop_frac = UrbanPop/100) %>%
  lm(Murder ~ UrbanPop_frac, data = .) %>%
  summary()

```

+ What happened to the intercept term and variance?

+ What happened to the slope coefficient, UrbanPop_frac relative to UrbanPop?

+ What happened to the regression summaries, $R^2$, F and such?


Try this again on your own but instead of multiplying something on the right hand side, I want you to add 5 to the murder rate, the LHS variable.

```{r}

```


+ What happened to the intercept term and variance?

+ What happened to the slope coefficient?

+ What happened to the regression summaries, $R^2$, F and such?


Try it again but this time change the murder rate from murders per 100,000 to murders per 1,000.

```{r}


```

+ What happened to the intercept term and variance?

+ What happened to the slope coefficient?

+ What happened to the regression summaries, $R^2$, F and such?


# Logs and Levels

Lets look at the simplest transformation, the log transform.  You should be interested in this for a few reasons, some statistical and some as a better way to link to theory.  Take a peek at the USArrests data.

```{r}
library(GGally)

USArrests %>%
  ggpairs()
```


Do you see how all the variables are roughly positively correlated?  Notice that a few may show a relationship that is increasing but at a decreasing rate.  That is the perfect circumstance to consider a log, or as we will later see a polynomial specification.  

Logs have the right characteristics.

$$\frac{\partial}{\partial x}ln(x) = \frac{1}{x}\\
\frac{\partial^2}{\partial x^2}ln(x) = - \frac{1}{x^2}
$$
The first derivative is positive and the second is negative, i.e., increasing at a decreasing rate.


Try looking the linear-linear relationship between Murder and Assault. A one unit increase in assault, 1 per 100,000, is related to a $\beta_1$ increase in assaults per 100,000.  
```{r}

```

Try looking the log-linear relationship between Murder and Assault. A one unit increase in assault, 1 per 100,000, is related to a $\beta_1$% increase in assaults per 100,000.  
```{r}

```


Try looking the linear-log relationship between Murder and Assault. A one percent increase in assaults per 100,000, is related to a $\beta_1$ increase in assaults per 100,000.  
```{r}

```


Try looking the log-log relationship between Murder and Assault. A one percent increase in assaults per 100,000, is related to a $\beta_1$% increase in assaults per 100,000.  

```{r}

```


# $Var(\hat{\beta})$ and $\hat{\sigma}^2$

# Regression through Origin: Our First Restriction

