---
title: "The Multiple Regression Inference"
author: "Jamie Woods"
date: ""
output: 
  html_notebook

runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidymodels)
library(car)
library(lmtest)


```


# The Assumptions and What it Buys You.

The hypotheses tests all depend on some assumptions about the residual terms. There are three major assumptions:  The residuals are normally distributed, the residuals are homoskedastic, there are lots and lots of observations.

Normally distributed:  The assumption you are making when you use your maximum likelihood estimator.  Generally, your $\hat{\beta}$ estimates are Normally distributed.

Homoscedasticity:  The residuals all have the same variance.  This is what you use with your least squares estimator.  Generally, your $\hat{\beta}$ estimates are distributed T, which gets closer and closer to Normal as the number of observations increase.

Lots of Observations:  These are the asymptotic properties in the chapter we skip.  The short reason this works is that so many things turn out to be Normally distributed its ridiculous.  The [Central Limit Theorems](https://en.wikipedia.org/wiki/Central_limit_theorem) are beautiful.


Don't let it freak you out that with the same data but that the coefficients that you estimate can have different distributions.  Your estimates are are combination of the data and the simplifying assumptions, including the functional form of your regression, you make.  These distributions are all contingent on the assumptions holding

## Normal/Homoskedastic Assumptions Don't Always Hold, But Are Close Enough.

We use the assumptions to derive the sampling distributions of your parameter estimates, the $\hat{\beta}$s.  Most of the time the assumptions are close enough but often don't hold _exactly_ for a variety of reasons.



+ Load the data and run this regression using `lm`

```{r}
library(datasets)

data("USArrests")

lm_murder_pop <- lm(Murder ~ UrbanPop, data = USArrests)

```


+ Plot $\sqrt{res}$ vs $\hat{y}$.  The $\sqrt{res}$ is meant to be like the standard deviation of the residuals.  The band should be the same for all $\hat{y}$. We did something similar in the estimation portion.  Is there a pattern?  Does it feel homoskedastic?

```{r}
library(tidyverse)

data.frame( yhat = predict(lm_murder_pop),
            sqrt_resid = sqrt( abs(residuals(lm_murder_pop))))  %>%
  ggplot( aes(y =sqrt_resid, x = yhat)) +
  geom_point() + 
  geom_smooth()

```


+ Test if they residuals are distributed normally  with Shapiro-Wilk, `shapiro.test`.  

```{r}

shapiro.test(residuals(lm_murder_pop))

```


I picked this test because it is very efficient. Keep in mind that as with most tests, when you add more data you will eventually reject the null hypothesis that the residuals are normally distributed.

Don't let this freak you out. You make assumptions to simplify computations and make it possible to draw inferences with smaller samples.  _One of your assumptions is your regression specification_.

Many apparent statistical problems, like the ones above, with your regressions are often just misspecification errors, like using a log when you shouldn't.  You can't fix a bad specification with more sophisticated statistics.

We have robust estimators, which adapt the confidence intervals after the fact for hetroskedasticity, `lm_robust` in the `library(estimatr)`package.  We will talk about this later.

Last big warning.  If you have enough data -- everything is``statistically significant``.  That does not mean it has a ``practical`` or ``economic significances``.  Watch the quotes.  The term ``significant`` and ``significance`` is emotionally loaded.

You -- knowing statistics -- may see both these as the same,``The program effect is insignificant``, ``The program effect is _statistically_ insignificant``, but the program manager heard that their program is insignificant.  They were just insulted and they have dug in their heels on feedback.

You can also have circumstances where the parameter estimate is statistically different from zero, you have rejected that null hypothesis, but that the program shows an increase in annual income of \$2.  \$2 is too little to have a practical or economic significance.  

Equivocation is evil.


# Testing with Individual $\hat{\beta}$

This is what you are normally dealing with, and is the most common test. You are interested is if you can reject the null hypothesis that $\hat{\beta} = 0$.  This is implicitly a two-sided test, meaning different than zero.

We have been reading our `summary(lm())` output at the p-values are listed for differences from zero.

You may want to test if a parameter is different from something other than zero.  This does happen.  I use some statistically adjusted engineering models, where the engineers make guesses about energy use by end-use, say cooking for example, and we observe total monthly use.  We test if the parameter estimates are different than 1.  Rejecting that null indicates that the engineers made bad guesses.

Just for kicks, lets check on our earlier murder regression and test if the parameter associated with UrbanPop, `r coef(lm_murder_pop)[2]`, is different from .05.

```{r}

# summary calculates a few values that we need.
lm_murder_summary <- summary(lm_murder_pop)

# Extract std error of estimate for beta
UrbanPop_se <- lm_murder_summary$coefficients["UrbanPop","Std. Error"]

# Find out how far away in standard deviations the estimate is from .05
t_05 <- (.05 - lm_murder_summary$coefficients["UrbanPop","Estimate"])/UrbanPop_se

# `pt` is the cdf of the t-distribution.  The t has a slightly different shape depending on the degrees of freedom, fewer means fatter tails.  The degrees of freedom in this case is residual degrees of freedom, observations less parameters in model.

pt(t_05, df = lm_murder_summary$df[2])


```


# Nested Tests?

Most of the time we will only be testing nested restrictions on the model.  What this means is that you have to be able to interpret your test as a restriction on you model.  Not including variables is the equivalent of restricting their coefficient estimates to zero.  You can also restrict coefficients to be the same and many other combinations.

Here are some examples.  Suppose you have a regression equation like this:

$$y = \alpha + \beta_1 x_1 + \beta_2 x_2 +\beta_3 x_3 +\beta_4 x_4$$

A nested test could be:

+ $\beta_1 = 0$, what your t-test usually does.
+ $\beta_1 = \beta_2 =0$, is a model without $x_1$ and $x_2$ different?
+ $\beta_2 = \beta_3$, are $\beta_2$ and $\beta_3$ different?

Each represents a restriction on the parameter estimates and we have to discover if the constraint is statistically significant.

There is some ability to perform non-nested tests, which we will see later. The inferences are not unequivocal.  You often find that two models each tell part of the story.

## F Tests

The basics of an F-test is that you are looking at two regressions, one that is not restricted and one that is.  The unrestricted one should have a smaller sum of squared residuals.  

You are checking to see if the restriction results in a statistically significant increase in residual sum of squares.  The F-test looks at the ratio of that difference  and the residual sum of squares of the unrestricted model.  _Basically its a statistical way of seeing if the percent change in the sum of squared residuals is statistically different from zero._  

The numerator and denominator are both distributed $\chi^2$, so the ratio is distributed F.  The degrees of freedom of the numerator is just the difference in residual degrees of freedom between the restricted and unrestricted model. The denominator has residual degrees of freedom as the unrestricted model.

You can also test with the inverse and some statistical software does that.  Just make sure to check both the value of the F-test and the p-value to make sure you are interpreting it correctly.

+ Create another murder regression but use Assault, UrbanPop and Rape to explain murder.  Save the result of that regression.

+ Do the same for a restricted model that does not include Assault and Rape.

+ Now perform an F-test, `anova(model2 , model1, test = "F" )` in R.  Was the restriction, NOT including the other variables, significant?

+ Now look at the F-test at the bottom of the summary output for model1.  That is a test between a model with only an intercept term and the model you specified.  What did you see?

## General Restrictions

You can do nested tests with simple zero restrictions with the F test.  This is what you do when you are including and excluding blocks of variables.  You can compare the different regression models with the `anova` function shown above or use the `linearHypothesis()` function in the `car` library.

# Non-Nested Tests

The book has some explanation of non-nested tests but there is an additional regression _based_ test that is very interesting.  Its called the J-Test and it allows you to compare two non-nested models.

Lets go back to our murder regressions and try two non-nested models.
```{r}
library(lmtest)
library(wooldridge)
data("USArrests")
lm_murder_pop <- lm(Murder ~ UrbanPop, data = USArrests)

lm_aug <- lm(Murder ~  Assault, data = USArrests)

```

You notice that the two models don't nest.  Both the models could be thought of as restriced versions of  `lm(Murder ~  Assault + UrbanPop, data = USArrests)`, but we aren't testing that right now.

What the J-Test does is that it augments each regression with the forecasted LHS of the other equation.  The chunk below adds the augmented forecasts of the LHS variable to the data frame.

```{r}
USArrests %>%
  mutate(Yhat_pop = predict(lm_murder_pop),
         Yhat_aug = predict(lm_aug)) -> USArrests

```


Then you run the augmented regressions, ignoring all the t-tests except the one associated with the forecast from the other model.

```{r}
lm_murder_pop_J <- lm(Murder ~ UrbanPop + Yhat_aug, data = USArrests)

lm_aug_J <- lm(Murder ~  Assault + Yhat_pop, data = USArrests)

```

Then look at the t-test for the forecast from the other variable.

```{r}
summary(lm_murder_pop_J)

```


```{r}
summary(lm_aug_J)
```
If the t-test is significant it says that you reject the null hypothesis that the other model had no additional explanatory power.  This is a cool situation where both models have something to teach the other.

You don't have to do the J-Test by hand.  There is a built-in function that does it.

```{r}
library(lmtest)

jtest(lm_murder_pop, lm_aug)
```

What is interesting about the J-Test is that it fails when the models _are_ nested.  Consider these two models similar to the ones above.

```{r}
lm_murder_pop <- lm(Murder ~ UrbanPop, data = USArrests)
lm_murder_unrestricted <- lm(Murder ~ UrbanPop + Assault, data = USArrests)

```

Notice that the model in lm_murder_pop is nested in lm_murder_unrestricted.  It just restricts the Assault parameter to zero. When you try to augment lm_murder_unrestricted with the forecast of the other model you are adding a variable that is a linear combination of the variables already included.  You just created perfect multicolinearity.  The test can't be computed because the regression can't be estimated.

```{r}
jtest(lm_murder_pop, lm_murder_unrestricted)
```

Notice that the forecast of the second model can be used in the first but the forecast of the first model can't be used in the second.  That new variable is just $6.41594 + 0.02093 UrbanPop$, a linear combination of the existing variables.

# Bootstrap Logic

Resampling is more common on the machine learning side, but is useful in econometrics too. The discussion below is about the naive bootstrap.  There are many bootstrap methods and specific problems you address with them.  The idea seems simple, because it is, and can help out sometime. 

Be very careful.  

The general idea of the bootstrap is that the population distribution is only directly observable through the samples that it gives you.  You can't ask for more, but you can pretend you can get new samples by resampling with replacement from the sample you have.

So, suppose you have observations from a die of 2, 3, 3, 4, 5, 6.  Note that we have not seen a 1.  Try the code below a few times.  You will get a simulated new sample by resampling the old.

```{r}
sample(c(2, 3, 3, 4, 5, 6 ), 6, replace = TRUE)

```



That's great, you can fake new samples but you will never have a resample with the number 1 in it since you didn't observe it in the original sample.  You can even end up with samples of all the same number.

Lets get a feel for how this works.  We are going to make some draws from a normal distribution.  Because we are in control of the data generating process, we will know the true population mean, zero, and standard deviation, 10.  What we want to know is what is the distribution of $\hat{\mu}$ our estimate of the mean of the distribution that we calculate from the sample.  

Bootstrapping can take a little time, so I added a busy spinner to let you know that this was actually working.

```{r echo=FALSE, warning=FALSE}
library(shiny)
library(ggplot2)
library(shinybusy)

map_mean <- function(split) mean(analysis(split)$x)



shinyApp(
  options = list(
    width = "100%", height = 600
  ),

  ui = fluidPage(
    sidebarLayout(
      sidebarPanel(

        sliderInput("sample_size", "Sample Size:", min = 2, max = 200, value = 20),

        sliderInput("boot_samples", "Resamples:", min = 1, max = 1000, value = 40, step = 20 )
      ),

      mainPanel(
        plotOutput("hist_plot_n_overlay")
      )
    ),
    add_busy_spinner(spin = "fading-circle")
  ),


  server = function(input, output) {

    simulated_data <- reactive({
      data.frame(x = rnorm(n = input$sample_size, mean = 0, sd = sqrt(10)))
    })

    boot_means <- reactive({
      set.seed(2354)
      boots <- bootstraps(simulated_data(), times = input$boot_samples, apparent = TRUE)

      boots %>%
        mutate(mean_est = map(splits, map_mean)) %>%
        select(mean_est) %>%
        unnest()
    })


  output$hist_plot_n_overlay =
    renderPlot({
     boot_means() %>%
        ggplot(aes(mean_est)) +
        geom_histogram() +
        stat_function(fun = dnorm, args = list(mean = mean(boot_means()$mean_est), sd = sd(boot_means()$mean_est)*input$boot_samples))
    })
  
  output$mean = renderPrint({
      head(boot_means(), 5)
    })
  }


)
```


Try this out.

+ Set a large sample size, 200, and vary the resamples around.  Can you think of different uses for small and large numbers of bootstrap replicates?

+ Set a small sample size and vary the number of bootstrap replicates


We can do something similar for regressions. This is called  bootstrapping the data.  The steps are:

+ Start with your data.
+ Decide on the number of times you will resample your data.  1000 is the most common number for confidence intervals.
+ Draw that many samples, _with replacement_, and run the same regression with each sample.
+ Record the $\hat{\beta}$ coefficients for each regression.  You can ignore the confidence intervals.
+  For your confidence intervals find the smallest interval that has $\alpha$ percent of the $\hat{\beta}$ estimates.


There is some R and Tidyverse infrastructure to this.  It is primarily designed for machine learning but you can use it in this context.

This sets up some infrastructure.

```{r}

library(tidymodels)

set.seed(612) # Do this so you get the same random samples every time (replicable)

# This draws a 1000 samples from the data with replacement.
boots <- bootstraps(USArrests, times = 1000, apparent = TRUE)

# Takes one of the bootstrap replicates from boots and gives back an lm model
lm_murder <- function(split) {
  lm(Murder ~ Assault  + UrbanPop, data = analysis(split))
}

```

This should take a few seconds to run.  Bootstrapping can take a _very_ long time.  Do yourself a favor and always store bootstrap iterations.

```{r}

boot_models <-
  boots %>%
  mutate(model = map(splits, lm_murder),    # Runs lm for each replicate
         coef_info = map(model, tidy))    # extracts the coefficients from each lm model

```

Take a look at the bootstrap confidence intervals.

```{r}
int_pctl(boot_models, coef_info)

```

Now compare to the OLS ones.

```{r}

USArrests %>%
  lm(Murder ~ Assault  + UrbanPop, data = . ) %>%
  confint()

```

Notice anything?  Remember that these are poorly specified models.  This may not be a statistical problem but a specification problem that looks like a statistical problem.

An experienced eye would see the difference being mostly in the intercept term as being evidence that you are missing a dummy variable -- a categorical attribute -- like high density.  I would take a look at the plots and see which states are causing the problem.


If you have a small enough data set, bootstraps are easy to do and don't take very long. *If you have a large number of observations, don't be surprised if it takes weeks.*

