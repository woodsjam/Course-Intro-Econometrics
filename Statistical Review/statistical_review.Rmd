---
title: "Statistical and Math Review"
author: "Your Name"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
runtime: shiny    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(PSUEC469) # Other libraries may be required.


set.seed(458) # make random results reproducible

```

# Summation Operator

Property 1

$$\sum_{i=1}^{n}c = nc$$
Property 2
$$\sum_{i=1}^{n}cx_i = c\sum_{i=1}^{n}x_i$$
Property 3
$$\sum_{i=1}^{n} (ax_i + b y_i) = a\sum_{i=1}^{n}x_i+ b\sum_{i=1}^{n}y_i$$

## Questions

- What is $\sum_{m=1}^{4} 8k -6m$?

$$\sum_{m=1}^{4} 8k -6m = \\$$

- Summation notation for $0 + .25 + .5 + .75$



- If the mean is $\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$ what is $\frac{1}{n} \sum_{i=1}^n (2x_i - \bar{x})$?

$$\frac{1}{n} \sum_{i=1}^n (2x_i - \bar{x}) = \\$$



- If the variance is defined as $\frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2$ what is the variance of 2x?

#  Logs

We will spend a lot of time in this class with the equations that are linear in the parameters but that can include polynomials, logs and other transformations like that.

A function that is linear in the parameters are additively separable where the parameters are just multiplied by some function of other variables but no other parameters.

These are all linear in the parameters:

+ $y = \beta_1 + \beta_2 x_1 + \beta_3 x_2$
+ $y = \beta_1 + \beta_2 x^2 + \beta_3 x^3$
+ $y = \beta_1 + \beta_2 \ln(x_1) + \beta_3 x_2$
+ $y = \beta_1 + \beta_2 x_1 + \beta_3 x_2x_1$

These are not:

+ $y = \frac{\beta_1}{\beta_2} +  \beta_1 x_1 + \beta_2 x_2$
+ $y = \beta_1 \beta_2 x_1 + \beta_1 x_2 + \beta_2 x_1$
+ $y = \beta_1 + x_1^{\beta_2}  + \beta_3 x_2$

We will often transform equations, by taking the natural log, which we just call the log, or square root to turn them into linear equations.


Logarithm rules:

+ $\ln{x y} = \ln{x} + \ln{y}$
+ $\ln{\frac{x}{y}} = \ln{x} - \ln{y}$
+ $\ln{x^y} = y\ln{x}$
+ $f(x) = \ln{x} \Rightarrow f'(x) = \frac{1}{x}$
+ $\lim_{x \rightarrow 0^+} = -\infty$
+ $\ln(1)=0$
+ Undefined at zero and complex valued when less than zero, $\ln(-1) = i\pi$.  We don't use complex values in this class.

Our equations will generally be polynomials and we will often be interested in the _marginal effects_ of a change in some variable.  This generally means taking the derivative with respect to that variable. I will keep things down to some simple cases but you should review these basic derivatives and the chain rule. I will use both the Lagrange, $f'$, and Leibniz, $\frac{d}{dx}$, notation interchangibly.

+ $\frac{d}{dx} C = 0$
+ $\frac{d}{dx} x^2 = 2 x$
+ $\frac{d}{dx} x^n = n x^{n-1}$
+ $\frac{d}{dx} e^x = e^x$
+ $\frac{d}{dx} e^{kx} = ke^{kx}$
+ $\frac{d}{dx} \ln{x} = \frac{1}{x}$
+ Chain rule $\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}$


## Questions

- $\ln(y) = \beta_1 z_1 + \beta z_2^2$ Is this linear in the parameters? Can it be linearized with a ln transform? If so, show.

- $y = \beta_1\beta_2x^{\beta_3}$ Is this linear in the parameters? Can it be linearized with a ln transform?  If so, show.


- Given $Income = 10 + .5 ~experience - .1 ~experince^2$, which explains income with years of experience, what is the marginal effect of a year of experience? 

- Given $Income = 10 + .5 ~experience - .1 ~experince^2$, which explains income with years of experience, when will income be maximized? 

- Given a demand function, $Q = e^{-5p} + 4$ what is the marginal effect of price changes?

- Given a demand function, $Q = e^{-5p} + 4$ at what price will people stop buying the good?

- Given a demand function, $Q = e^{p^2-5p} + 4$ what is the marginal effect of price changes?

# Probability Discrete and Continuous

We will spend most of our time in this class with a few distributions, Normal, T, $\chi^2$, and F but we will be using the language of joint, marginal and conditional distributions.

## Marginal, Conditional and Joint

Since the math requirements for this class are just calc 1, we will not do any of this with continuous distributions, like the normal distribution but with discrete distributions.

Lets start with the following joint distribution of X and Y, $p(x,y)$, that allows X and Y to each take on integer values from 1 to 3.  The Xs are the columns and the Ys are the rows.

|  |1 |2 |3 |
|---|---|---|---|
|1 |.1|.05|.3|
|2 |.05|.1| .1|
|3 |.05|.2| .05|

### Questions

This kind of depends on you remembering this from your stats classes or that you have reviewed appendixes A, B and C.

-  Does this make sense as a joint distribution?  How can you tell?

- What is the probability of y being equal to 1 and x is equal to 3, $p(y =1, x =3)$?

- What is the probability that x and y are the same?

- What is the marginal distribution of x, $p(x)$

- What is the conditional distribution of x given that y is 2, $p(x|y = 2)$



## Expectations, Variance and other Statistics


Add up, a literal sum or integration, the value of a random variable times the probability of seeing that value. Variance is similar except that the value you are looking at is the square of the difference between the value and the mean.


Expectations are the easiest to think about.  If you have a random variable X the expected value, remember this is about the population, not the sample, is $\sum_{i =1}^n a_i f_x(x_i)$ with a reasonable extension to joint distributions $$\sum_{h =1}^k   \sum_{k =1}^m  a(x_h, y_k) f_{X,Y}(x_h,y_k)$$



|  |1 |2 |3 |
|---|---|---|---|
|1 |.1|.05|.3|
|2 |.05|.1| .1|
|3 |.05|.2| .05|


+ What is E[X]?

+ What is E[X|Y= 1]?

There are a few properties to remember about expectations and variances. Covariance, $E[(X- \mu_x)(Y - \mu_y)]$ is basically the same as variance except with two variables rather than one.  We often talk about the variance covariance matrix because of this.

+ $E[a + b X] = a + b E[X]$ Slide left and right with a  and scale with b.
+ $ Var[a + b X] = b^2 Var[X]$ Sliding does not matter.
+ $Var(a X + b Y) = a^2 Var(X) + b^2 Var(Y) - 2abCov(X,Y)$  The last term is the key.  Think of the variance of two die rolls added together and the variance of one die doubled, i.e., $cor(X,Y) = 1 = \frac{Cov(X,Y)}{sd(X) sd(Y)}$.


- What is the $cov(X,Y)$?
- What is the expected value of $X+Y$? There are a few ways of doing this.



## The Big Three Distributions and Where They Come From

The normal distribution, to quote Frank Samaniego, is a mathematical gift.  So many things so many things are normally distributed and so many statistics, single number summaries of many numbers, are also normally distributed we start there.

There are a few laws of large numbers.  The main use we are making of them right now is to realize that we are estimating the mean of a random variable.  It does not matter if the random variable is Normally distributed, or Gamma, or anything, as long as the variance of the distribution, not the sample, is finite, your estimate of the mean will be normally distributed.

The finite variance requirement is odd but there are distributions, Cauchy comes to mind, that have no defined variance, meaning infinite.  Visually, if you take a normal and make the tails fatter and fatter, shrinking the mode down, you end up with a Cauchy.  This will be a key issue when you take the time-series econometrics class.

The next big distribution is the T, which you get when you are trying to estimate the mean of a distribution and the variance at the same time.  The tails are a little fatter.  Some of our parameter estimates, which are conditional means, are distributed T.  Sometimes, with different assumptions, they are normal.

The $\chi^2_n$ distribution is formed from the sum of the squares of normal random variables.  The, $n$, called the degrees of freedom, tells you the number of terms you are adding up.

$$\chi^2_n = \sum_{i = 1}^n N(0,1)^2$$


Note that squares of random variables happen quite a bit. recall that variance is, $\sum_{i =1}^n (x_i - \bar{x})^2$  That is the sum of n random variables. Just about any time you see a squared term think $\chi^2$.

The last big one for us is the F distribution.  This is formed by the ratio of two normalized $\chi^2$ variables and we generally use it to see if two variances are different than each other.  

The F distribution has two degrees of freedom measures, one for the numerator and one for the denominator $F(d_1, d_2) = \frac{U_1/d_1}{U_2/d_2}$.  Dividing by the degrees of freedom, i.e., the number of squared normals you add up, you get the distribution back on the scale of 1 rather than d, which is the mean of a $\chi^2$.

The quick list:

+ When in doubt, it's probably normal.
+ If you add up squared normals, its probably $\ch^2$.
+ If you are comparing variances, you are comparing two sums of squared normals and therefore dealing with an F distribution.

Don't get too confident.  The [wikipedia page](https://en.wikipedia.org/wiki/List_of_probability_distributions) that lists the probability distributions is very long.